import datetime
import utils as ut
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Lambda, Conv2D, Dense, Flatten
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def get_data(csv_file_path, test_size=0.2):
    '''
    Loads data in from a csv file and outputs the resulting training and validation images 
    and labels.

    csv_file_path -> full file path to the csv file holding the data
    test_size     -> Percentage of data set aside for validation (float between 0. and 1.)
    
    returns training and validation data (X_train, X_valid, y_train, y_valid) where:
        X_train/X_valid -> N x 3 matrices that contain image names for the center, left, and right columns
        y_train/y_valid -> N x 1 matrices that contain every steering angle

    NOTE:
    
    The csv file should contain data in the following order:
    
    Center Image, Left Image, Right Image, Steering Angle, Throttle, Break, and Speed
    0,            1,          2,           3,              4,        5,         6
    '''
    # Load in file with pandas
    df = pd.read_csv(csv_file_path, ",",
                    names=["center", "left", "right", "steering", "throttle", "break", "speed"])

    # Specify our training images and their labels 
    X_df = df[['center', 'left', 'right']].values
    y_df = df['steering'].values

    # Split our data into training and validation sets and return the result
    return train_test_split(X_df, y_df, test_size=test_size)

def create_model(input_shape, normalization=lambda x: x):
    '''
    Creates a modified version of the NVIDIA Model outlined in the post found @ https://devblogs.nvidia.com/deep-learning-self-driving-cars/

    input_shape   -> shape of the input to the model
    normalization -> function that normalizes input fed to the model

    returns a Keras Model

    '''
    # Create Sequential Model
    model = Sequential()

    # Feed Input to Model and Normalize the Images
    model.add(Lambda(normalization, input_shape=input_shape))
    
    # Three convolutional layers with a 2×2 stride and a 5×5 kernel
    model.add(Conv2D(filters=24, kernel_size=5, strides=2, padding='valid', activation='relu'))
    model.add(Conv2D(filters=36, kernel_size=5, strides=2, padding='valid', activation='relu'))
    model.add(Conv2D(filters=48, kernel_size=5, strides=2, padding='valid', activation='relu'))

    # Two non-strided convolution with a 3×3 kernel size
    model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu'))
    model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu'))

    # Flatten
    model.add(Flatten())

    # Three fully connected layers
    model.add(Dense(100, activation='relu'))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(10))
    
    # Final connected layer
    model.add(Dense(1))

    return model

def train_model(model, X_train, X_valid, y_train, y_valid, batch_size=32, epochs=3, save_model_path="model.h5"):
    '''
    Trains a model with an Adam optimizer, a Mean Squared Error loss function, and with 
    training and validation data generated by the utility image_data_batch_generator function.

    model           -> a Keras model
    X_train/X_valid -> N x 3 matrices that contain image names for the center, left, and right columns
    y_train/y_valid -> N x 1 matrices that contain every steering angle
    batch_size      -> how large is each batch for each epoch
    epochs          -> the number of epochs used to train this model
    save_model_path -> string that specifies the file where the resulting model weights will be saved

    returns a history object of the trained model

    '''
    # finalize model and specify loss and optimizer functions
    model.compile(loss='mse', optimizer='adam')
    # create training and validation generators
    train_gen = ut.image_data_batch_generator(X=X_train, y=y_train, 
                                            batch_size=batch_size, 
                                            prepreprocessing=ut.training_preprocessing)
    valid_gen = ut.image_data_batch_generator(X=X_valid, y=y_valid, 
                                            batch_size=batch_size, 
                                            prepreprocessing=ut.validation_preprocessing)
    # train the model with the training and validation generators
    # Note: the generators should not manipulate the size of the data!
    history_object = model.fit_generator(train_gen, 
                                steps_per_epoch=len(y_train),
                                validation_data=valid_gen, 
                                validation_steps=len(y_valid), 
                                epochs=epochs,
                                verbose=1)

    # save the model
    model.save(save_model_path)

    # return model training history
    return history_object

'''

Main

'''

# General Setup
csvFilePath = ut.DATA_PATH + "driving_log.csv"
visualizingData = False
normalization = lambda x: x/127.5 - 1.
epochs = 3
arch_title = '"NVIDIA Architecture"'
changes = '"Adding Valuable Input Data."'

# Get Training and Validation Data
X_train, X_valid, y_train, y_valid = get_data(csvFilePath)

# Visualize Data
if visualizingData:
    # Visualize Data Before and After Preprocessing then Exit
    # Pandas Histogram Plotting Tutorial: https://realpython.com/python-histograms/

    # Visualize Training Data Distribution Before and After Augmentation
    y_train_pp = next(ut.image_data_batch_generator(X=X_train, y=y_train, 
                                prepreprocessing=ut.training_preprocessing,
                                batch_size=len(y_train)))[1]

    print("Original Data Variance  = ", np.var(np.array(y_train, dtype=np.float64)))
    print("Augmented Data Variance = ", np.var(np.array(y_train_pp, dtype=np.float64)))

    ut.save_hist(pd.Series(np.array(y_train)), 
                title='Training Set Steering Angle Distribution', 
                xlabel='Steering Angle', 
                ylabel='Count', 
                bins=20, 
                save_as='images/angle_distribution.jpg')

    ut.save_hist(pd.Series(np.array(y_train_pp)), 
                title='Augmented Training Set Steering Angle Distribution', 
                xlabel='Steering Angle', 
                ylabel='Count', 
                bins=20, 
                save_as='images/augmented_angle_distribution.jpg')

    # Visualize Before and After of Several Preprocessed Images
    count = 3

    X_train_images = [ut.read_image(ut.filename_from_path(paths[0])) for i, paths in enumerate(X_train) if i < count]
    X_train_images_pp = [ut.preprocess_image(img) for img in X_train_images]

    ut.plotImages(images=X_train_images + X_train_images_pp, 
                titles=['Before', 'After'], 
                columns=count, 
                row_titles=True, 
                save_as='images/before_and_after_preprocessing.jpg')

    exit()

# Create NN Model To Train On
model = create_model(input_shape=ut.NVIDIA_INPUT_SHAPE, normalization=normalization)

# Train Model
saveModelPath = str("model_" + datetime.datetime.now().strftime('%Y-%m-%d_%H_%M_%S') + ".h5")
history_object = train_model(model, 
                            X_train, X_valid, y_train, y_valid, 
                            save_model_path=saveModelPath,
                            epochs=epochs)

# Update Log
ut.update_log(history_object=history_object, 
           batch_size=32,
           arch_title=arch_title, 
           changes=changes)

print("Saved: ", saveModelPath)
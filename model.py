import datetime
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Lambda, Conv2D, Dense, Flatten
from sklearn.model_selection import train_test_split
from utils import image_data_batch_generator, DATA_PATH, update_log

def get_data(csv_file_path, test_size=0.2):
    '''
    Loads data in from a csv file and outputs the resulting training and validation images 
    and labels.

    csv_file_path -> full file path to the csv file holding the data
    test_size     -> Percentage of data set aside for validation (float between 0. and 1.)
    
    returns training and validation data (X_train, X_valid, y_train, y_valid) where:
        X_train/X_valid -> N x 3 matrices that contain image names for the center, left, and right columns
        y_train/y_valid -> N x 1 matrices that contain every steering angle

    NOTE:
    
    The csv file should contain data in the following order:
    
    Center Image, Left Image, Right Image, Steering Angle, Throttle, Break, and Speed
    0,            1,          2,           3,              4,        5,         6
    '''
    # Load in file with pandas
    df = pd.read_csv(csv_file_path, ",",
                    names=["center", "left", "right", "steering", "throttle", "break", "speed"])

    # Specify our training images and their labels 
    X_df = df[['center', 'left', 'right']].values
    y_df = df['steering'].values

    # Split our data into training and validation sets and return the result
    return train_test_split(X_df, y_df, test_size=test_size)

def create_model(input_shape, normalization=lambda x: x):
    '''
    Creates a modified version of the NVIDIA Model outlined in the post found @ https://devblogs.nvidia.com/deep-learning-self-driving-cars/

    input_shape   -> shape of the input to the model
    normalization -> function that normalizes input fed to the model

    returns a Keras Model

    '''
    # Create Sequential Model
    model = Sequential()

    # Feed Input to Model and Normalize the Images
    model.add(Lambda(normalization, input_shape=input_shape))
    
    # Three convolutional layers with a 2×2 stride and a 5×5 kernel
    model.add(Conv2D(filters=24, kernel_size=5, strides=2, padding='valid', activation='relu'))
    model.add(Conv2D(filters=36, kernel_size=5, strides=2, padding='valid', activation='relu'))
    model.add(Conv2D(filters=48, kernel_size=5, strides=2, padding='valid', activation='relu'))

    # Two non-strided convolution with a 3×3 kernel size
    model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu'))
    model.add(Conv2D(filters=64, kernel_size=3, padding='valid', activation='relu'))

    # Flatten
    model.add(Flatten())

    # Three fully connected layers
    model.add(Dense(100, activation='relu'))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(10))
    
    # Final connected layer
    model.add(Dense(1))

    return model

def train_model(model, X_train, X_valid, y_train, y_valid, batch_size=32, epochs=3, save_model_path="model.h5"):
    '''
    Trains a model with an Adam optimizer, a Mean Squared Error loss function, and with 
    training and validation data generated by the utility image_data_batch_generator function.

    model           -> a Keras model
    X_train/X_valid -> N x 3 matrices that contain image names for the center, left, and right columns
    y_train/y_valid -> N x 1 matrices that contain every steering angle
    batch_size      -> how large is each batch for each epoch
    epochs          -> the number of epochs used to train this model
    save_model_path -> string that specifies the file where the resulting model weights will be saved

    returns a history object of the trained model

    '''
    # finalize model and specify loss and optimizer functions
    model.compile(loss='mse', optimizer='adam')
    # train model with generators
    history_object = model.fit_generator(image_data_batch_generator(X=X_train, y=y_train, batch_size=batch_size), 
                                steps_per_epoch=len(y_train),
                                validation_data=image_data_batch_generator(X=X_valid, y=y_valid, batch_size=batch_size), 
                                validation_steps=len(y_valid), 
                                epochs=epochs,
                                verbose=1)

    # save the model
    model.save(save_model_path)

    # return model training history
    return history_object

'''

Main

'''

# General Setup
csvFilePath = DATA_PATH + "driving_log.csv"
visualizingData = False
normalization = lambda x: x/127.5 - 1.
input_shape = (160, 320, 3)

# Get Training and Validation Data
X_train, X_valid, y_train, y_valid = get_data(csvFilePath)

# Visualize Data
if visualizingData:
    # TODO Visualize Data Before and After Preprocessing 
    # Then Exit
    exit()
    pass

# Create NN Model To Train On
model = create_model(input_shape=input_shape, normalization=normalization)

# Train Model
saveModelPath = str("model_" + datetime.datetime.now().strftime('%Y-%m-%d_%H_%M_%S') + ".h5")
history_object = train_model(model, X_train, X_valid, y_train, y_valid, save_model_path=saveModelPath)

# Update Log
update_log(history_object=history_object, 
           batch_size=32,
           arch_title='"NVIDIA Architecture"', 
           changes='"First Run"')

print("Saved: ", saveModelPath)